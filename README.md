#  ETL Pipeline

The goal of the project was to create a basic ETL pipeline using pandas. The tasks involved in the project was to extract a data set stored in a csv format and do some filtering after which the extracted data will be stored in parquet and json(gzip) format. Final file format where uploaded to AWS S3 bucket.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.

### Prerequisites

What things you need to run the script.

* Python: for the data manipulation.
* Gitbash: For pushing the files your repo.
* AWS(S3): For hosting the extracted files.
* An IDE: To run and edit the codes

### Installing

A step by step series of examples that tell you how to get a development env running

* Install python on your system(macOS, windows or Linux)
* Using pip and the [requirement file](https://drive.google.com/file/d/14mlEM-5mbhD4oQpF8fLCZXKZaLlTPo-C/view?usp=sharing), use the below command to install the required dependencies.
* Clone repo.

```
pip install -r requirement.txt
```

## Acknowledgments

* Hisham Osman
