# Basic ETL Pipeline

The goal of the project was to create a basic ETL pipeline using. The tasks involved in the project was to extract a data set stored in a csv format and do some filtering after which the extracted data will be stored in parquet and json(gzip) format. Final file format where uploaded to AWS S3 bucket.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.

### Prerequisites

What things you need to install the software and how to install them

* Python: for the data manipulation.
* Gitbash: For pushing the files your repo.
* AWS(S3): For hosting the extracted files.

### Installing

A step by step series of examples that tell you how to get a development env running

* Install python on your system(macOS, windows or Linux)
* Using pip and the requirement file, use the below command to install the required dependencies.

```
pip install -r requirement.txt
```

## Acknowledgments

* Hisham Osman
